{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:00.215932Z",
     "start_time": "2024-08-11T18:38:00.210682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "def load_local_data():\n",
    "\n",
    "    file_paths = {\n",
    "        \"train\": {\"en\": \"../data/nejm.train.en\", \"zh\": \"../data/nejm.train.zh\"},\n",
    "        \"dev\": {\"en\": \"../data/nejm.dev.en\", \"zh\": \"../data/nejm.dev.zh\"},\n",
    "        \"test\": {\"en\": \"../data/nejm.test.en\", \"zh\": \"../data/nejm.test.zh\"}\n",
    "    }\n",
    "    # Define a dictionary to hold file paths\n",
    "\n",
    "    data = {}\n",
    "    for split, paths in file_paths.items():\n",
    "        # Loop through the dictionary to get the corresponding dataset and paths\n",
    "\n",
    "        with open(paths['en'], encoding='utf-8') as f_en, open(paths['zh'], encoding='utf-8') as f_zh:\n",
    "            en_lines = f_en.readlines()\n",
    "            zh_lines = f_zh.readlines()\n",
    "\n",
    "        examples = [{\"translation\": {\"en\": en.strip(), \"zh\": zh.strip()}} for en, zh in zip(en_lines, zh_lines)]\n",
    "        data[split] = Dataset.from_dict({\"translation\": examples})\n",
    "        # Extract each corresponding English and Chinese line from the three datasets (train, dev, test),\n",
    "        # strip any surrounding whitespace, and construct a dataset object for each split.\n",
    "\n",
    "    return DatasetDict(data)\n"
   ],
   "id": "85ce9b57520e914d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:03.058861Z",
     "start_time": "2024-08-11T18:38:00.360224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "raw_datasets = load_local_data()\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "# Load the dataset and evaluation metric"
   ],
   "id": "d5dcd6f1e6f88d89",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:03.066039Z",
     "start_time": "2024-08-11T18:38:03.059860Z"
    }
   },
   "cell_type": "code",
   "source": "raw_datasets",
   "id": "6123c82152ef8c93",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 62127\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2036\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['translation'],\n",
       "        num_rows: 2102\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:03.101723Z",
     "start_time": "2024-08-11T18:38:03.096117Z"
    }
   },
   "cell_type": "code",
   "source": "metric",
   "id": "89d3fc251aa7f34f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
       "Produces BLEU scores along with its sufficient statistics\n",
       "from a source against one or more references.\n",
       "\n",
       "Args:\n",
       "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
       "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
       "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
       "        - `'none'`: no smoothing\n",
       "        - `'floor'`: increment zero counts\n",
       "        - `'add-k'`: increment num/denom by k for n>1\n",
       "        - `'exp'`: exponential decay\n",
       "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
       "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
       "        - `'none'`: No tokenization.\n",
       "        - `'zh'`: Chinese tokenization.\n",
       "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
       "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
       "        - `'char'`: Language-agnostic character-level tokenization.\n",
       "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
       "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
       "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
       "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
       "\n",
       "Returns:\n",
       "    'score': BLEU score,\n",
       "    'counts': Counts,\n",
       "    'totals': Totals,\n",
       "    'precisions': Precisions,\n",
       "    'bp': Brevity penalty,\n",
       "    'sys_len': predictions length,\n",
       "    'ref_len': reference length,\n",
       "\n",
       "Examples:\n",
       "\n",
       "    Example 1:\n",
       "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        100.0\n",
       "\n",
       "    Example 2:\n",
       "        >>> predictions = [\"hello there general kenobi\",\n",
       "        ...                 \"on our way to ankh morpork\"]\n",
       "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
       "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
       "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
       "        >>> results = sacrebleu.compute(predictions=predictions,\n",
       "        ...                             references=references)\n",
       "        >>> print(list(results.keys()))\n",
       "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
       "        >>> print(round(results[\"score\"], 1))\n",
       "        39.8\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:04.817752Z",
     "start_time": "2024-08-11T18:38:03.102724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# Define the pre-trained model to be fine-tuned and its tokenizer\n",
    "\n",
    "max_word_length = 128\n",
    "input_lang = \"en\"\n",
    "output_lang = \"zh\"\n",
    "# Set the maximum sequence length and specify the languages for the dataset"
   ],
   "id": "e2d9eb7a0cccc835",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:04.823366Z",
     "start_time": "2024-08-11T18:38:04.818121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def token_f(examples):\n",
    "    inputs = [ex[\"translation\"][input_lang] for ex in examples[\"translation\"]]\n",
    "    outputs = [ex[\"translation\"][output_lang] for ex in examples[\"translation\"]]\n",
    "    # Extract all the English and Chinese sentences from the corresponding dataset\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=max_word_length, truncation=True)\n",
    "    # Use the tokenizer to automatically tokenize the English inputs, truncating if they exceed 128 tokens\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(outputs, max_length=max_word_length, truncation=True)\n",
    "        labels[\"input_ids\"] = [[token for token in label if token != 8] for label in labels[\"input_ids\"]]\n",
    "    # Switch to the target language (Chinese) tokenizer for tokenization, truncating if it exceeds 128 tokens.\n",
    "    # Since this method can insert padding tokens (spaces) between processed Chinese tokens (related to data), \n",
    "    # we manually remove all padding tokens.\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    # Add the processed target language token IDs to the 'labels' key in the input dictionary\n",
    "\n",
    "    return model_inputs"
   ],
   "id": "29e679ffb24d2521",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:04.831773Z",
     "start_time": "2024-08-11T18:38:04.824366Z"
    }
   },
   "cell_type": "code",
   "source": "token_f(raw_datasets[\"train\"][:2])",
   "id": "eb62d8f469ad2738",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[5961, 56, 8, 37, 1550, 11931, 22602, 1766, 4, 1541, 1582, 11, 5119, 44083, 1669, 3196, 8, 6, 0], [1557, 8, 3376, 16, 3376, 465, 2686, 4, 42731, 3196, 22, 98, 2725, 149, 38435, 5097, 8, 17, 32, 59, 435, 30222, 695, 4, 37399, 9608, 8, 6, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[7057, 10373, 37, 1390, 12363, 33392, 16334, 9019, 24383, 106, 2039, 4734, 33164, 26456, 199, 1402, 21767, 6, 0], [67, 22249, 2554, 1189, 12, 42544, 42508, 13859, 1098, 295, 848, 3940, 12, 1729, 5160, 2068, 6, 0]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:04.837194Z",
     "start_time": "2024-08-11T18:38:04.832774Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_id_to_find = [7057, 10373, 37, 1390, 12363, 33392, 16334, 9019, 24383, 106, 2039, 4734, 33164, 26456, 199, 1402, 21767, 6, 0]\n",
    "token_str = tokenizer.convert_ids_to_tokens(token_id_to_find)\n",
    "print(token_str,end='')"
   ],
   "id": "112c476fae6abfbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁也许', '▁不能', ':', '分析', '▁结果', '提示', '激', '素', '疗法', '▁在', '维持', '▁去', '脂', '体重', '方面', '作用', '很小', '.', '</s>']"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:04.842604Z",
     "start_time": "2024-08-11T18:38:04.838193Z"
    }
   },
   "cell_type": "code",
   "source": [
    "token_id_to_find = [5961, 56, 8, 37, 1550, 11931, 22602, 1766, 4, 1541, 1582, 11, 5119, 44083, 1669, 3196, 8, 6, 0]\n",
    "token_str = tokenizer.convert_ids_to_tokens(token_id_to_find)\n",
    "print(token_str,end='')"
   ],
   "id": "8aab8c4cb99a4ab3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁probably', '▁not', '▁', ':', '▁analysis', '▁suggests', '▁minimal', '▁effect', '▁of', '▁H', 'T', '▁in', '▁maintaining', '▁lean', '▁body', '▁mass', '▁', '.', '</s>']"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:26.567268Z",
     "start_time": "2024-08-11T18:38:04.844602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = raw_datasets.map(token_f, batched=True)\n",
    "# Process each sample in the dataset using the token_f function and enable batched processing"
   ],
   "id": "14b8fa099e131e76",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/62127 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "54f91b9770014ef59702312c316324ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2036 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2b871ef07d9449fd917c3aa220c69c9e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2102 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5db083c318fa4dfb856ede7a6e00dade"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:29.002581Z",
     "start_time": "2024-08-11T18:38:26.568270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "# Load the defined pre-trained model"
   ],
   "id": "6f50812b023ac526",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:30.195596Z",
     "start_time": "2024-08-11T18:38:29.003250Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"translation_dir\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=5,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False,\n",
    ")\n",
    "# Configure the training parameters for the model. \n",
    "# Setting fp16 to False mainly ensures stability and precision."
   ],
   "id": "323ddec9a75069fb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:30.199696Z",
     "start_time": "2024-08-11T18:38:30.196598Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "# Define a data collator to pad and align the data before passing it to the model"
   ],
   "id": "caf3fcfd59958407",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:38:30.205271Z",
     "start_time": "2024-08-11T18:38:30.200702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_text(preds, labels):\n",
    "\n",
    "    decoded_preds = [pred.split() for pred in preds]\n",
    "    decoded_labels = [label.split() for label in labels]\n",
    "    pred = [pred.strip() for pred in decoded_preds]\n",
    "    label = [label.strip() for label in decoded_labels]\n",
    "    # Split sentences into lists and remove any surrounding whitespace\n",
    "\n",
    "    return pred, label"
   ],
   "id": "98355a2193f42d87",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:52:50.152888Z",
     "start_time": "2024-08-11T18:52:50.145938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    # Decode the predicted sequences into Chinese, ignoring special tokens\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    # During training, padding tokens are marked with -100 to be ignored by the model,\n",
    "    # so we need to convert these -100 markers back to the corresponding padding token ID.\n",
    "\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    # Decode the true sequences for BLEU score calculation\n",
    "\n",
    "    decoded_preds = [pred.split() for pred in decoded_preds]\n",
    "    decoded_labels = [label.split() for label in decoded_labels]\n",
    "    # Split the decoded sentences into lists of words\n",
    "\n",
    "    decoded_preds, decoded_labels = process_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=[[dl] for dl in decoded_labels])\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    # Calculate the BLEU score\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    # Calculate the average generated length, ignoring padding tokens\n",
    "\n",
    "    return result"
   ],
   "id": "d94154fbb46e8518",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T18:52:50.536814Z",
     "start_time": "2024-08-11T18:52:50.461137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"dev\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "# Define the training process"
   ],
   "id": "6e77021eb97b07bc",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-11T22:07:08.404551Z",
     "start_time": "2024-08-11T18:52:51.352216Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "2043a9550e512d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19415' max='19415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19415/19415 3:14:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.186200</td>\n",
       "      <td>1.115826</td>\n",
       "      <td>31.696818</td>\n",
       "      <td>38.078585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.019800</td>\n",
       "      <td>1.033558</td>\n",
       "      <td>32.705277</td>\n",
       "      <td>37.960707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.915700</td>\n",
       "      <td>0.998998</td>\n",
       "      <td>33.534264</td>\n",
       "      <td>38.028978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.855200</td>\n",
       "      <td>0.983651</td>\n",
       "      <td>33.690372</td>\n",
       "      <td>38.041257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.823500</td>\n",
       "      <td>0.979454</td>\n",
       "      <td>33.624423</td>\n",
       "      <td>37.871316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=19415, training_loss=1.0095811803102064, metrics={'train_runtime': 11656.0171, 'train_samples_per_second': 26.65, 'train_steps_per_second': 1.666, 'total_flos': 8115711890817024.0, 'train_loss': 1.0095811803102064, 'epoch': 5.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-12T04:39:12.644924Z",
     "start_time": "2024-08-12T04:39:10.232317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_path = \"translation_dir/checkpoint-19000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "# Define the fine-tuned model to use, load its tokenizer and model\n",
    "\n",
    "def translate_to_chinese(text):\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # Convert the input text into a tensor of token IDs\n",
    "\n",
    "    translated = model.generate(**inputs)\n",
    "    # Pass the dictionary of inputs to the model using ** unpacking,\n",
    "    # which makes it easier to add parameters later without changing the function manually\n",
    "\n",
    "    translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "    # Take the highest-scoring sequence, i.e., the most likely sequence, and ignore special tokens\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "english_text = \"the maximum tolerated dose of asciminib was not reached.\"\n",
    "chinese_translation = translate_to_chinese(english_text)\n",
    "chinese_sentence = chinese_translation.replace(\" \", \"\")\n",
    "# Remove unnecessary spaces from the translation\n",
    "\n",
    "print(chinese_sentence)"
   ],
   "id": "3a373a6e7b2a3c91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未达到阿司匹尼的最大耐受剂量.\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
